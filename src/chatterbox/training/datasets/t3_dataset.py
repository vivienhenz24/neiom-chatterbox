"""
Dataset utilities for reading S3/tokenized training samples generated by the
Luxembourgish T3 preparation pipeline.
"""
from __future__ import annotations

import csv
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Collection, Mapping, Optional

import torch
from torch.utils.data import Dataset

from ...models.t3.modules.t3_config import T3Config


_DEFAULT_T3_HP = T3Config.multilingual()


@dataclass(frozen=True)
class T3DatasetStats:
    """Summary statistics for a token dataset on disk."""

    total_files: int
    kept_samples: int
    dropped_for_length: int
    dropped_for_language: int
    dropped_missing_text: int
    languages: tuple[str, ...]
    max_speech_tokens: int
    max_text_tokens: Optional[int]
    avg_speech_tokens: float
    avg_text_tokens: Optional[float]


@dataclass(frozen=True)
class _SampleIndex:
    """Lightweight record describing an available token sample."""

    file_path: Path
    sample_id: str
    speech_len: int
    text_len: Optional[int]
    language_id: Optional[str]
    audio_path: Optional[str]


class _StatsAccumulator:
    """Internal helper used while indexing the dataset."""

    def __init__(self, total_files: int) -> None:
        self.total_files = total_files
        self.kept_samples = 0
        self.dropped_for_length = 0
        self.dropped_for_language = 0
        self.dropped_missing_text = 0
        self.languages: set[str] = set()
        self.max_speech_tokens = 0
        self.max_text_tokens: Optional[int] = None
        self._sum_speech_tokens = 0
        self._sum_text_tokens = 0
        self._count_text_tokens = 0

    def record_keep(self, speech_len: int, text_len: Optional[int], language_id: Optional[str]) -> None:
        self.kept_samples += 1
        self._sum_speech_tokens += speech_len
        self.max_speech_tokens = max(self.max_speech_tokens, speech_len)
        if text_len is not None:
            self._count_text_tokens += 1
            self._sum_text_tokens += text_len
            if self.max_text_tokens is None:
                self.max_text_tokens = text_len
            else:
                self.max_text_tokens = max(self.max_text_tokens, text_len)
        if language_id is not None:
            self.languages.add(language_id)

    def record_length_drop(self) -> None:
        self.dropped_for_length += 1

    def record_language_drop(self) -> None:
        self.dropped_for_language += 1

    def record_missing_text_drop(self) -> None:
        self.dropped_missing_text += 1

    def to_stats(self) -> T3DatasetStats:
        avg_speech = self._sum_speech_tokens / self.kept_samples if self.kept_samples else 0.0
        avg_text: Optional[float] = None
        if self._count_text_tokens:
            avg_text = self._sum_text_tokens / self._count_text_tokens

        return T3DatasetStats(
            total_files=self.total_files,
            kept_samples=self.kept_samples,
            dropped_for_length=self.dropped_for_length,
            dropped_for_language=self.dropped_for_language,
            dropped_missing_text=self.dropped_missing_text,
            languages=tuple(sorted(self.languages)),
            max_speech_tokens=self.max_speech_tokens,
            max_text_tokens=self.max_text_tokens,
            avg_speech_tokens=avg_speech,
            avg_text_tokens=avg_text,
        )


class T3TokenDataset(Dataset):
    """
    Lazy dataset for speech/text token files produced by the orchestration pipeline.

    Parameters
    ----------
    token_dir:
        Directory containing ``*.pt`` files generated by
        :func:`chatterbox.training.data_preparation.run_preparation`.
    dataset_root:
        Optional base directory used to resolve relative ``audio_path`` entries embedded
        in token files. When omitted, audio paths are returned exactly as saved.
    metadata:
        Optional mapping keyed by sample id (derived from the token filename stem)
        providing additional metadata to merge into every sample.
    metadata_resolver:
        Optional callable that receives a sample id and should return a metadata mapping
        (or ``None`` if no metadata is available). When both a mapping and resolver are
        supplied, their results are merged with resolver output taking precedence.
    max_speech_len / max_text_len:
        Skip samples whose token sequence length exceeds the provided thresholds.
        Text filtering is only applied when a sample contains text tokens.
    drop_missing_text:
        When ``True``, discard samples that lack text tokens.
    allowed_languages:
        Optional whitelist of language ids (case-insensitive). Samples whose language id
        is missing or not present in the whitelist are dropped.

    Notes
    -----
    - All tensors remain on CPU. Transfer to another device is expected to happen in the
      training loop/collate function.
    - Token files are only materialized when accessed via ``__getitem__``; only metadata
      required for filtering and statistics is cached in memory.
    """

    def __init__(
        self,
        token_dir: Path | str,
        *,
        dataset_root: Path | str | None = None,
        metadata: Mapping[str, Mapping[str, Any]] | None = None,
        metadata_resolver: Callable[[str], Optional[Mapping[str, Any]]] | None = None,
        max_speech_len: int | None = None,
        max_text_len: int | None = None,
        drop_missing_text: bool = False,
        allowed_languages: Collection[str] | None = None,
        start_text_token: int | None = None,
        stop_text_token: int | None = None,
        start_speech_token: int | None = None,
        stop_speech_token: int | None = None,
    ) -> None:
        super().__init__()

        self._token_dir = Path(token_dir).expanduser().resolve()
        if not self._token_dir.exists():
            raise FileNotFoundError(f"Token directory not found: {self._token_dir}")
        if not self._token_dir.is_dir():
            raise NotADirectoryError(f"Token path is not a directory: {self._token_dir}")

        self._dataset_root = Path(dataset_root).expanduser().resolve() if dataset_root is not None else None

        self._metadata_map = dict(metadata) if metadata is not None else None
        self._metadata_resolver = metadata_resolver

        self._max_speech_len = max_speech_len
        self._max_text_len = max_text_len
        self._drop_missing_text = drop_missing_text
        self._allowed_languages = {lang.lower() for lang in allowed_languages} if allowed_languages else None

        self._start_text_token = start_text_token if start_text_token is not None else _DEFAULT_T3_HP.start_text_token
        self._stop_text_token = stop_text_token if stop_text_token is not None else _DEFAULT_T3_HP.stop_text_token
        self._start_speech_token = (
            start_speech_token if start_speech_token is not None else _DEFAULT_T3_HP.start_speech_token
        )
        self._stop_speech_token = (
            stop_speech_token if stop_speech_token is not None else _DEFAULT_T3_HP.stop_speech_token
        )

        token_files = sorted(self._token_dir.glob("*.pt"))
        if not token_files:
            raise FileNotFoundError(f"No token files (*.pt) found under {self._token_dir}")

        stats_acc = _StatsAccumulator(total_files=len(token_files))
        entries: list[_SampleIndex] = []
        for token_file in token_files:
            record = self._inspect_token_file(
                token_file,
                max_speech_len=self._max_speech_len,
                max_text_len=self._max_text_len,
                drop_missing_text=self._drop_missing_text,
                allowed_languages=self._allowed_languages,
                stats=stats_acc,
            )
            if record is not None:
                entries.append(record)

        if not entries:
            raise RuntimeError(
                "All token files were filtered out. "
                "Relax the filtering constraints (max lengths, language whitelist, missing text policy)."
            )

        self._entries = entries
        self._stats = stats_acc.to_stats()

    @property
    def token_dir(self) -> Path:
        """Directory containing the token files."""
        return self._token_dir

    @property
    def dataset_root(self) -> Optional[Path]:
        """Optional dataset root used to resolve audio paths."""
        return self._dataset_root

    @property
    def stats(self) -> T3DatasetStats:
        """Return cached dataset statistics."""
        return self._stats

    def __len__(self) -> int:
        return len(self._entries)

    def __getitem__(self, index: int) -> dict[str, Any]:
        info = self._entries[index]
        try:
            payload = torch.load(info.file_path, map_location="cpu")
        except FileNotFoundError as exc:
            raise FileNotFoundError(f"Token file missing for sample '{info.sample_id}': {info.file_path}") from exc
        except Exception as exc:  # pylint: disable=broad-except
            raise RuntimeError(f"Failed to load token file {info.file_path}: {exc}") from exc

        speech_tokens = payload.get("speech_tokens")
        if speech_tokens is None:
            raise KeyError(f"'speech_tokens' missing from token file {info.file_path}")
        if not isinstance(speech_tokens, torch.Tensor):
            raise TypeError(f"'speech_tokens' in {info.file_path} must be a torch.Tensor, got {type(speech_tokens)!r}")
        speech_tokens = speech_tokens.to("cpu")
        if speech_tokens.dim() > 1:
            speech_tokens = speech_tokens.view(-1)

        speech_len = payload.get("speech_token_len", info.speech_len)
        speech_len = int(speech_len)
        if speech_len > speech_tokens.numel():
            speech_len = int(speech_tokens.numel())
        speech_tokens = speech_tokens[:speech_len].to(dtype=torch.long)

        tokens_segments = []
        if speech_tokens.numel() == 0 or speech_tokens[0].item() != self._start_speech_token:
            tokens_segments.append(torch.tensor([self._start_speech_token], dtype=torch.long))
        tokens_segments.append(speech_tokens)
        if speech_tokens.numel() == 0 or speech_tokens[-1].item() != self._stop_speech_token:
            tokens_segments.append(torch.tensor([self._stop_speech_token], dtype=torch.long))
        speech_tokens = torch.cat(tokens_segments)
        speech_len = int(speech_tokens.numel())

        text_tokens = payload.get("text_tokens")
        if text_tokens is not None:
            if not isinstance(text_tokens, torch.Tensor):
                raise TypeError(f"'text_tokens' in {info.file_path} must be a torch.Tensor, got {type(text_tokens)!r}")
            text_tokens = text_tokens.to("cpu")

            if text_tokens.dim() == 0:
                text_tokens = text_tokens.unsqueeze(0)

        text_len = payload.get("text_token_len", info.text_len)
        if text_tokens is not None and text_len is None:
            # Fallback when length metadata was not persisted.
            text_len = int(text_tokens.numel())
        text_len = int(text_len) if text_len is not None else None

        if text_tokens is not None:
            if text_tokens.dtype != torch.long:
                text_tokens = text_tokens.to(dtype=torch.long)
            if text_tokens.numel() == 0:
                text_tokens = torch.tensor([self._start_text_token, self._stop_text_token], dtype=torch.long)
            else:
                if text_tokens[0].item() != self._start_text_token:
                    text_tokens = torch.cat(
                        (torch.tensor([self._start_text_token], dtype=torch.long), text_tokens),
                        dim=0,
                    )
                if text_tokens[-1].item() != self._stop_text_token:
                    text_tokens = torch.cat(
                        (text_tokens, torch.tensor([self._stop_text_token], dtype=torch.long)),
                        dim=0,
                    )
            text_len = int(text_tokens.numel())

        language_id = payload.get("language_id", info.language_id)
        if isinstance(language_id, str):
            language_id = language_id.lower()
        else:
            language_id = None

        audio_rel = payload.get("audio_path", info.audio_path)
        audio_path: Optional[Path] = None
        if audio_rel is not None:
            if not isinstance(audio_rel, str):
                raise TypeError(f"'audio_path' in {info.file_path} must be a string, got {type(audio_rel)!r}")
            audio_path = Path(audio_rel)
            if self._dataset_root is not None:
                candidate = (self._dataset_root / audio_path).resolve()
                if not candidate.exists():
                    raise FileNotFoundError(f"Referenced audio file not found: {candidate}")
                audio_path = candidate

        known_fields = {
            "speech_tokens",
            "speech_token_len",
            "text_tokens",
            "text_token_len",
            "audio_path",
            "language_id",
        }
        metadata = {k: v for k, v in payload.items() if k not in known_fields}
        external_metadata = self._resolve_metadata(info.sample_id)
        if external_metadata:
            metadata.update(external_metadata)

        return {
            "id": info.sample_id,
            "speech_tokens": speech_tokens,
            "speech_token_len": speech_len,
            "text_tokens": text_tokens,
            "text_token_len": text_len,
            "audio_path": audio_path,
            "language_id": language_id,
            "metadata": metadata or None,
        }

    def _resolve_metadata(self, sample_id: str) -> Optional[Mapping[str, Any]]:
        mapping_value = self._metadata_map.get(sample_id) if self._metadata_map is not None else None
        resolver_value = self._metadata_resolver(sample_id) if self._metadata_resolver is not None else None

        if mapping_value and resolver_value:
            merged = dict(mapping_value)
            merged.update(resolver_value)
            return merged
        return resolver_value or mapping_value

    def _inspect_token_file(
        self,
        token_file: Path,
        *,
        max_speech_len: Optional[int],
        max_text_len: Optional[int],
        drop_missing_text: bool,
        allowed_languages: Optional[Collection[str]],
        stats: _StatsAccumulator,
    ) -> Optional[_SampleIndex]:
        try:
            payload = torch.load(token_file, map_location="cpu")
        except FileNotFoundError as exc:
            raise FileNotFoundError(f"Token file disappeared during indexing: {token_file}") from exc
        except Exception as exc:  # pylint: disable=broad-except
            raise RuntimeError(f"Failed to read token file {token_file}: {exc}") from exc

        speech_tokens = payload.get("speech_tokens")
        speech_len = payload.get("speech_token_len")
        if speech_tokens is None or speech_len is None:
            raise KeyError(f"'speech_tokens' and 'speech_token_len' are required in {token_file}")
        if not isinstance(speech_tokens, torch.Tensor):
            raise TypeError(f"'speech_tokens' in {token_file} must be a torch.Tensor, got {type(speech_tokens)!r}")
        speech_len = int(speech_len)
        if speech_len <= 0:
            raise ValueError(f"Invalid 'speech_token_len' ({speech_len}) in {token_file}")
        if speech_tokens.dim() > 1:
            speech_tokens = speech_tokens.view(-1)
        speech_tokens = speech_tokens.to(dtype=torch.long)
        if speech_len > speech_tokens.numel():
            speech_len = int(speech_tokens.numel())

        needs_start = speech_len == 0 or speech_tokens[0].item() != self._start_speech_token
        needs_stop = speech_len == 0 or speech_tokens[min(speech_len - 1, speech_tokens.numel() - 1)].item() != self._stop_speech_token
        adjusted_speech_len = speech_len + int(needs_start) + int(needs_stop)

        text_tokens = payload.get("text_tokens")
        text_len = payload.get("text_token_len")
        if text_tokens is not None and not isinstance(text_tokens, torch.Tensor):
            raise TypeError(f"'text_tokens' in {token_file} must be a torch.Tensor, got {type(text_tokens)!r}")
        has_text_tokens = text_tokens is not None
        if has_text_tokens:
            if text_len is None:
                text_len = int(text_tokens.numel())
            else:
                text_len = int(text_len)

        language_id = payload.get("language_id")
        language_norm: Optional[str]
        if isinstance(language_id, str):
            language_norm = language_id.lower()
        elif language_id is None:
            language_norm = None
        else:
            raise TypeError(f"'language_id' in {token_file} must be a string or None, got {type(language_id)!r}")

        if max_speech_len is not None and adjusted_speech_len > max_speech_len:
            stats.record_length_drop()
            return None
        if max_text_len is not None and text_len is not None and text_len > max_text_len:
            stats.record_length_drop()
            return None
        if drop_missing_text and not has_text_tokens:
            stats.record_missing_text_drop()
            return None
        if allowed_languages is not None and language_norm not in allowed_languages:
            stats.record_language_drop()
            return None

        audio_path = payload.get("audio_path")
        if audio_path is not None and not isinstance(audio_path, str):
            raise TypeError(f"'audio_path' in {token_file} must be a string, got {type(audio_path)!r}")

        stats.record_keep(
            speech_len=adjusted_speech_len,
            text_len=text_len if has_text_tokens else None,
            language_id=language_norm,
        )

        sample_index = _SampleIndex(
            file_path=token_file,
            sample_id=token_file.stem,
            speech_len=adjusted_speech_len,
            text_len=text_len if has_text_tokens else None,
            language_id=language_norm,
            audio_path=audio_path,
        )

        # Release tensors promptly before moving to the next file.
        del payload, speech_tokens, text_tokens

        return sample_index

    @staticmethod
    def load_metadata_csv(
        path: Path | str,
        *,
        key_field: str = "path",
        encoding: str = "utf-8",
    ) -> dict[str, dict[str, str]]:
        """
        Convenience helper that parses a metadata CSV and returns a mapping usable with
        ``metadata=``.

        Parameters
        ----------
        path:
            Path to the CSV file (typically ``metadata.csv`` alongside the dataset).
        key_field:
            Column containing the audio path or unique identifier. The filename stem is
            used as the sample id.
        encoding:
            Encoding used to read the CSV file.
        """
        csv_path = Path(path).expanduser().resolve()
        if not csv_path.exists():
            raise FileNotFoundError(f"Metadata CSV not found: {csv_path}")

        with csv_path.open("r", newline="", encoding=encoding) as handle:
            reader = csv.DictReader(handle)
            if reader.fieldnames is None or key_field not in reader.fieldnames:
                raise KeyError(f"Column '{key_field}' not found in metadata CSV: {csv_path}")

            result: dict[str, dict[str, str]] = {}
            for row in reader:
                key_value = row.get(key_field)
                if not key_value:
                    continue
                sample_id = Path(key_value).stem
                result[sample_id] = dict(row)
            return result


# Public re-export for type checking / convenience.
__all__ = ["T3TokenDataset", "T3DatasetStats"]
