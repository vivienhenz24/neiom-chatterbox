dataset:
  train_tokens_dir: "data/luxembourgish_tokens/train"
  valid_tokens_dir: "data/luxembourgish_tokens/test"
  batch_size: 8
  eval_batch_size: 8
  num_workers: 8
  max_source_tokens: null
  max_target_tokens: null

model:
  base_checkpoint: "models/multilingual/t3_mtl23ls_v2.safetensors"
  freeze_encoder: false
  freeze_decoder: false
  freeze_modules: []

optimizer:
  name: adamw
  lr: 2.0e-5
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01

scheduler:
  name: cosine
  warmup_steps: 1000
  min_lr: 5.0e-6

training:
  epochs: 8
  gradient_accumulation_steps: 4
  mixed_precision: true
  max_grad_norm: 1.0
  eval_every_n_steps: 250

logging:
  output_dir: "runs/runpod_luxembourgish"
  log_every_n_steps: 50
  tensorboard_enabled: true
  wandb_enabled: false
  wandb_project: null
  wandb_run_name: null
  checkpoint_every_n_steps: 1000
  max_checkpoints: 4

seed:
  python: 1337
  numpy: null
  torch: null
